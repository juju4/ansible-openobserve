{{ ansible_managed | comment }}

receivers:
  journald:
    directory: /var/log/journal
  filelog/std:
    include: {{ openobserve_agent_filelog_include | string }}
    # start_at: beginning
  hostmetrics:
    root_path: /
    collection_interval: 30s
    scrapers:
      cpu:
      disk:
      filesystem:
      load:
      memory:
      network:
      paging:
      processes:
      # process: # a bug in the process scraper causes the collector to throw errors so disabling it for now
processors:
  resourcedetection/system:
    detectors: ["system"]
    system:
      hostname_sources: ["os"]
  memory_limiter:
    check_interval: 1s
    limit_percentage: 75
    spike_limit_percentage: 15
  batch:
    send_batch_size: 10000
    timeout: 10s

extensions:
  zpages: {}

exporters:
  otlphttp/openobserve:
    endpoint: {{ openobserve_agent_server_url }}
{% if openobserve_agent_auth_key is defined and openobserve_agent_auth_key != None and openobserve_agent_auth_key | length > 0 %}
    headers:
      Authorization: "Basic {{ openobserve_agent_auth_key }}"
{% endif %}
  otlphttp/openobserve_journald:
    endpoint: {{ openobserve_agent_server_url }}
    headers:
{% if openobserve_agent_auth_key is defined and openobserve_agent_auth_key != None and openobserve_agent_auth_key | length > 0 %}
      Authorization: "Basic {{ openobserve_agent_auth_key }}"
{% endif %}
      stream-name: journald

service:
  extensions: [zpages]
  pipelines:
    metrics:
      receivers: [hostmetrics]
      processors: [resourcedetection/system, memory_limiter, batch]
      exporters: [otlphttp/openobserve]
    logs:
      receivers: [filelog/std]
      processors: [resourcedetection/system, memory_limiter, batch]
      exporters: [otlphttp/openobserve]
    logs/journald:
      receivers: [journald]
      processors: [resourcedetection/system, memory_limiter, batch]
      exporters: [otlphttp/openobserve_journald]
